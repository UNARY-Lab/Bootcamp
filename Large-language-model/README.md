# Large language model

## Reading
Transformer
1. [Paper](https://arxiv.org/pdf/2207.09238.pdf)
2. [Blog](https://towardsdatascience.com/attention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634)
3. [Tutorial](https://e2eml.school/transformers.html)
4. [Blogger](https://jalammar.github.io/illustrated-transformer/)
5. [Video](https://www.youtube.com/watch?v=rBCqOTEfxvg)

## Optimization
[General reading](https://towardsdatascience.com/demystifying-efficient-self-attention-b3de61b9b0fb)
[Good practice](https://arxiv.org/abs/2211.05102)

KV cache
1. [Tutorial 1](https://medium.com/@joaolages/kv-caching-explained-276520203249)
2. [Tutorial 2](https://lilianweng.github.io/posts/2023-01-10-inference-optimization)
3. [Tutorial 3](https://kipp.ly/transformer-inference-arithmetic)
4. [Tutorial 4](https://www.dipkumar.dev/becoming-the-unbeatable/posts/gpt-kvcache)
5. [Paper 1](https://arxiv.org/abs/2303.06865)
6. [Paper 2](https://arxiv.org/abs/2309.06180)
   
Self-attention mask
1. [Tutorial 1](https://gmongaras.medium.com/how-do-self-attention-masks-work-72ed9382510f)
2. [Code 1](https://github.com/facebookresearch/llama/blob/main/llama/model.py#L300)

Weight only quantization
1. [Tutorial](https://medium.com/intel-analytics-software/effective-weight-only-quantization-for-large-language-models-with-intel-neural-compressor-39cbcb199144)
2. [Paper 1](https://arxiv.org/abs/2308.09723)
3. [Paper 2](https://arxiv.org/abs/2306.00978)
4. [Paper 3](https://arxiv.org/abs/2312.08583)
5. [Code 1](https://github.com/casper-hansen/AutoAWQ)
6. [Code 2](https://github.com/jerry-chee/QuIP)

## Hardware
[Survey](https://arxiv.org/abs/2302.14017)

